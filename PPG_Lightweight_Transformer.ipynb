{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+ZzlM4GwuARmwIO7kXcon",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eric-rWang/VivoX/blob/main/PPG_Lightweight_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lightweight Transformer Implementation"
      ],
      "metadata": {
        "id": "yVAlVS-Ko9Rw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugXQ1rxxo7eO",
        "outputId": "8067f3fe-bc68-4504-f1bb-d12f8e017c4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DATA\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "import os\n",
        "import h5py\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "os.chdir(\"/content/DATA\")\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architecture:**\n",
        "\n",
        "**Input:**\n",
        "(batch_size, 1, 36, window_size) (36 channels: 12 locations × 3 wavelengths)\n",
        "\n",
        "**Channel-wise Embedding:**\n",
        "Stack of 1D convolutions and pooling compresses each channel’s temporal waveform into a single 256-dimensional feature vector.\n",
        "\n",
        "**Transformer Encoder:**\n",
        "A 2-layer Transformer encoder (with 4 attention heads) models relationships between these compressed features.\n",
        "\n",
        "**Regression Head:**\n",
        "Fully connected layers map the output to 2 values (SvO2 and SaO2).\n",
        "\n",
        "**Flow:**\n",
        "Input → Conv1d/Pooling → Feature Vector → Transformer → Output Head → Prediction"
      ],
      "metadata": {
        "id": "fcdq4Y6gpGyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightweightTransformer(nn.Module):\n",
        "    def __init__(self, num_channels=36, window_size=350):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Channel-wise embedding with proper output dimension\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Conv1d(num_channels, 64, kernel_size=7, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "\n",
        "        # 2. Transformer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=256,\n",
        "            nhead=4,\n",
        "            dim_feedforward=512,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "\n",
        "        # 3. Output head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "        # Separate heads for SaO2 and SvO2\n",
        "        self.head_sa = nn.Sequential(\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        self.head_sv = nn.Sequential(\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (B, 1, C, T)\n",
        "        x = x.squeeze(1)  # (B, C, T)\n",
        "        x = self.embed(x)  # (B, 256, 1)\n",
        "        x = x.squeeze(-1)  # (B, 256)\n",
        "        x = x.unsqueeze(1)  # (B, 1, 256)\n",
        "        x = self.transformer(x)  # (B, 1, 256)\n",
        "        x = x.squeeze(1)  # (B, 256)\n",
        "\n",
        "        return self.head(x)\n",
        "\n",
        "        # sa_pred = self.head_sa(x)  # (B, 1)\n",
        "        # sv_pred = self.head_sv(x)  # (B, 1)\n",
        "        # out = torch.cat([sa_pred, sv_pred], dim=1)  # (B, 2)\n",
        "        # return out"
      ],
      "metadata": {
        "id": "VxX9dA5IpK8d"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supporting functions for script\n"
      ],
      "metadata": {
        "id": "SKq9w6V_ptQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_model(model):\n",
        "    model.eval()\n",
        "    try:\n",
        "        # Test with random data matching our expected input shape\n",
        "        dummy_input = torch.randn(2, 1, 36, 350)\n",
        "        output = model(dummy_input)\n",
        "        assert output.shape == (2, 2), f\"Bad output shape: {output.shape}\"\n",
        "        print(f\"Model verification passed\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Model verification failed: {str(e)}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "FSuM4ExkpwHU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def import_h5py(file_path):\n",
        "  with h5py.File(file_path, 'r') as f:\n",
        "      X = f['waveforms'][:]\n",
        "      y = f['labels'][:]\n",
        "\n",
        "      return X, y"
      ],
      "metadata": {
        "id": "bsxrijWIpzd7"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurations"
      ],
      "metadata": {
        "id": "xHt8fR0aqB65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_SEPARATE_TEST_FILE = True  # Set to False to use single file approach\n",
        "\n",
        "if USE_SEPARATE_TEST_FILE:\n",
        "    train_val_file_path = \"Shifting_Positions_v4.0.h5\"\n",
        "    test_file_path = \"Validation_v1.0.h5\"\n",
        "    print(f\"Loading training/validation data from {train_val_file_path} ...\")\n",
        "    print(f\"Loading test data from {test_file_path} ...\")\n",
        "else:\n",
        "    file_path = \"/content/DATA/jul14th_shift_sensor_data_eric.h5\"\n",
        "    print(f\"Loading data from {file_path} ...\")\n",
        "\n",
        "save_dir = 'saved_models'\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWbL8jV1qD5C",
        "outputId": "77db0445-dd46-408d-92fd-bdbc0f6eea33"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training/validation data from Shifting_Positions_v4.0.h5 ...\n",
            "Loading test data from Validation_v1.0.h5 ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model hyperparameters"
      ],
      "metadata": {
        "id": "0zaWh6NxqJp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_channels = 36\n",
        "window_size = 350\n",
        "batch_size = 32\n",
        "lr = 1e-4\n",
        "weight_decay = 1e-4\n",
        "num_epochs = 15 # was 300\n",
        "n_patience = 50\n",
        "\n",
        "# Models to train\n",
        "MODELS = {\n",
        "    \"LightTransformer\": LightweightTransformer\n",
        "}"
      ],
      "metadata": {
        "id": "PhjULJ8CqL6F"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loading and preparation"
      ],
      "metadata": {
        "id": "Wqz8KUYSqWuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data Loading and Preparation ---\n",
        "if USE_SEPARATE_TEST_FILE:\n",
        "    # Load training/validation data\n",
        "    # Ensure we are in the correct directory before loading\n",
        "    os.chdir(\"/content/DATA\")\n",
        "    train_val_file_path = \"Shifting_Position_v4.0.h5\"\n",
        "    combined_data, labels_array = import_h5py(train_val_file_path)\n",
        "    print(f\"Training/Val data loaded ✅\\nData shape: {combined_data.shape}, Labels shape: {labels_array.shape}\")\n",
        "\n",
        "    # Load test data separately\n",
        "    test_file_path = \"Validation_v1.0.h5\"\n",
        "    X_test, y_test = import_h5py(test_file_path)\n",
        "    # X_test, y_test = import_h5py(test_file_path)\n",
        "    print(f\"Test data loaded ✅\\nData shape: {X_test.shape}, Labels shape: {y_test.shape}\")\n",
        "\n",
        "    # Split the training/validation data into train and val (75/25 split)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        combined_data, labels_array,\n",
        "        test_size=0.25,\n",
        "        random_state=42\n",
        "    )\n",
        "else:\n",
        "    # Original single file approach\n",
        "    combined_data, labels_array = import_h5py(file_path)\n",
        "    print(f\"Loaded ✅\\nData shape: {combined_data.shape}, Labels shape: {labels_array.shape}\")\n",
        "\n",
        "    # Use 100% of combined_data: 60% train, 20% val, 20% test\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        combined_data, labels_array,\n",
        "        train_size=0.6,\n",
        "        random_state=42\n",
        "    )\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp,\n",
        "        test_size=0.5,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "print(\"Training data range:\")\n",
        "print(f\"X_train: {X_train.min()} to {X_train.max()}\")\n",
        "print(f\"y_train: {y_train.min()} to {y_train.max()}\")\n",
        "print(\"\\nTest data range:\")\n",
        "print(f\"X_test: {X_test.min()} to {X_test.max()}\")\n",
        "print(f\"y_test: {y_test.min()} to {y_test.max()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4pexdl4qYpP",
        "outputId": "d6ed5a08-2280-4b43-a513-034f2890a29f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training/Val data loaded ✅\n",
            "Data shape: (9940, 350, 36), Labels shape: (9940, 2)\n",
            "Test data loaded ✅\n",
            "Data shape: (915, 350, 36), Labels shape: (915, 2)\n",
            "Training data range:\n",
            "X_train: -173630.13649154917 to 213508.44104482824\n",
            "y_train: 30 to 100\n",
            "\n",
            "Test data range:\n",
            "X_test: -10221.409632908988 to 9173.120933216334\n",
            "y_test: 30 to 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diagnostic"
      ],
      "metadata": {
        "id": "hUViPrxGqnff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnostic: original train‐set class counts\n",
        "print(f\"\\nOriginal training set class counts:\")\n",
        "labels, counts = np.unique(y_train, axis=0, return_counts=True)\n",
        "for lbl, cnt in zip(labels, counts):\n",
        "    print(f\"  {lbl.tolist()}: {cnt} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0eF0YPJqpD6",
        "outputId": "a9210360-5aa3-42f2-d91d-73b82474a2b3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original training set class counts:\n",
            "  [100, 30]: 1714 samples\n",
            "  [100, 40]: 1352 samples\n",
            "  [100, 50]: 1350 samples\n",
            "  [100, 60]: 1335 samples\n",
            "  [100, 70]: 1704 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balancing"
      ],
      "metadata": {
        "id": "LXZZNnANqq8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Balance TRAINING SET only via median‐based under/oversampling ---\n",
        "target = int(np.median(counts))\n",
        "print(f\"\\nBalancing training set to {target} samples per class (median count)\")\n",
        "balanced_X, balanced_y = [], []\n",
        "\n",
        "def jitter(x, σ=0.02):\n",
        "    return x + np.random.normal(0, σ*np.std(x), size=x.shape)\n",
        "def time_shift(x, max_shift=15):\n",
        "    s = np.random.randint(-max_shift, max_shift+1)\n",
        "    return np.roll(x, s, axis=0)\n",
        "\n",
        "for lbl, cnt in zip(labels, counts):\n",
        "    idxs = np.where((y_train == lbl).all(axis=1))[0]\n",
        "    X_lbl = X_train[idxs]\n",
        "    y_lbl = y_train[idxs]\n",
        "    # undersample if too big\n",
        "    if cnt > target:\n",
        "        chosen = np.random.choice(idxs, target, replace=False)\n",
        "        balanced_X.append(X_train[chosen])\n",
        "        balanced_y.append(y_train[chosen])\n",
        "    # oversample + augment if too small\n",
        "    elif cnt < target:\n",
        "        balanced_X.append(X_lbl)\n",
        "        balanced_y.append(y_lbl)\n",
        "        n_to_gen = target - cnt\n",
        "        for _ in range(n_to_gen):\n",
        "            i = np.random.choice(idxs)\n",
        "            x_aug = time_shift(jitter(X_train[i]))\n",
        "            balanced_X.append(x_aug[None])\n",
        "            balanced_y.append(lbl[None])\n",
        "    else:\n",
        "        balanced_X.append(X_lbl)\n",
        "        balanced_y.append(y_lbl)\n",
        "\n",
        "# concatenate back\n",
        "X_train = np.vstack(balanced_X)\n",
        "y_train = np.vstack(balanced_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EUkx-XGqsT9",
        "outputId": "6cc55b0a-6ee9-491d-c2f0-fe840397414e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Balancing training set to 1352 samples per class (median count)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post balancing diagnostic"
      ],
      "metadata": {
        "id": "mB8hBmpkq9Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnostic: neue training und test set Klassenquantitäten\n",
        "print(\"Post‐balance training set class counts:\")\n",
        "new_labels, new_counts = np.unique(y_train, axis=0, return_counts=True)\n",
        "for lbl, old_cnt, new_cnt in zip(labels, counts, new_counts):\n",
        "    delta = new_cnt - old_cnt\n",
        "    pct = delta/old_cnt*100\n",
        "    print(f\"  {lbl.tolist()}: {new_cnt} samples ({'+' if delta>=0 else ''}{delta}, {pct:.1f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vHYEncBq4sh",
        "outputId": "9d1f4b39-5f47-4e89-895c-85acffca9b68"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Post‐balance training set class counts:\n",
            "  [100, 30]: 1352 samples (-362, -21.1%)\n",
            "  [100, 40]: 1352 samples (+0, 0.0%)\n",
            "  [100, 50]: 1352 samples (+2, 0.1%)\n",
            "  [100, 60]: 1352 samples (+17, 1.3%)\n",
            "  [100, 70]: 1352 samples (-352, -20.7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization\n"
      ],
      "metadata": {
        "id": "vfr2KE_zq_ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Better input normalization (per‐channel)\n",
        "def normalize_ppg(X):\n",
        "    median = np.median(X, axis=2, keepdims=True)\n",
        "    mad    = 1.4826 * np.median(np.abs(X - median), axis=2, keepdims=True)\n",
        "    return (X - median) / (mad + 1e-6)\n",
        "\n",
        "def global_normalize_ppg(X):\n",
        "    \"\"\"\n",
        "    Normalize each sample in X using global min-max across all channels and time points.\n",
        "    Args:\n",
        "        X: np.ndarray of shape (N, C, T)\n",
        "    Returns:\n",
        "        X_norm: np.ndarray of shape (N, C, T)\n",
        "    \"\"\"\n",
        "    X_min = X.min(axis=(1, 2), keepdims=True)  # shape (N, 1, 1)\n",
        "    X_max = X.max(axis=(1, 2), keepdims=True)  # shape (N, 1, 1)\n",
        "    X_norm = (X - X_min) / (X_max - X_min + 1e-6)\n",
        "    return X_norm\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "X_train = global_normalize_ppg(X_train)\n",
        "X_val   = global_normalize_ppg(X_val)\n",
        "X_test  = global_normalize_ppg(X_test)\n",
        "\n",
        "# Label normalization\n",
        "y_max   = 100.0\n",
        "y_train = y_train / y_max\n",
        "y_val   = y_val   / y_max\n",
        "y_test  = y_test  / y_max\n",
        "\n",
        "print(\"Post-scaling X range:\", X_train.min(), X_train.max())\n",
        "print(\"Post-scaling y range:\", y_train.min(), y_train.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWtsBszLrDrA",
        "outputId": "f879591c-9a68-4d7a-8fb0-75e01e72b4b7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6760, 350, 36)\n",
            "(2485, 350, 36)\n",
            "(915, 350, 36)\n",
            "Post-scaling X range: 0.0 0.9999999999970896\n",
            "Post-scaling y range: 0.3 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the splits for later\n",
        "np.savez(os.path.join(save_dir, 'data_splits.npz'),\n",
        "         X_train=X_train, y_train=y_train,\n",
        "         X_val=X_val,     y_val=y_val,\n",
        "         X_test=X_test,   y_test=y_test)"
      ],
      "metadata": {
        "id": "Mpc5sJQRrGPW"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to tensors and reshape\n",
        "X_train_t = torch.from_numpy(X_train).float()\n",
        "y_train_t = torch.from_numpy(y_train).float()\n",
        "X_val_t = torch.from_numpy(X_val).float()\n",
        "y_val_t = torch.from_numpy(y_val).float()\n",
        "X_test_t = torch.from_numpy(X_test).float()\n",
        "y_test_t = torch.from_numpy(y_test).float()"
      ],
      "metadata": {
        "id": "9rvTSN8zrKGi"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape to (N, 1, 36, 350)\n",
        "X_train_rs = X_train_t.permute(0, 2, 1).unsqueeze(1)\n",
        "X_val_rs = X_val_t.permute(0, 2, 1).unsqueeze(1)\n",
        "X_test_rs = X_test_t.permute(0, 2, 1).unsqueeze(1)"
      ],
      "metadata": {
        "id": "VQbdQyCUrPsL"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize using only training statistics und so\n",
        "mean = X_train_rs.mean(dim=(0, 1, 3), keepdim=True)\n",
        "std = X_train_rs.std(dim=(0, 1, 3), keepdim=True)\n",
        "X_train_rs = (X_train_rs - mean) / (std + 1e-6)\n",
        "X_val_rs = (X_val_rs - mean) / (std + 1e-6)\n",
        "X_test_rs = (X_test_rs - mean) / (std + 1e-6)"
      ],
      "metadata": {
        "id": "YeuI_JVJrRr1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoaders\n",
        "train_loader = DataLoader(TensorDataset(X_train_rs, y_train_t), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(X_val_rs, y_val_t), batch_size=batch_size)\n",
        "test_loader = DataLoader(TensorDataset(X_test_rs, y_test_t), batch_size=batch_size)"
      ],
      "metadata": {
        "id": "GuAsOMMjrTPS"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Loop für alle Modelle ---\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(f\"🚀 Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    print(\"🍏 Using Apple Silicon GPU (MPS)\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"💀 Falling back to CPU? Yikes!\")\n",
        "\n",
        "for model_name, model_class in MODELS.items():\n",
        "\n",
        "    print(f\"\\n=== Training {model_name} ===\")\n",
        "    model = model_class(\n",
        "        num_channels=num_channels,\n",
        "        window_size=window_size\n",
        "    ).to(device)\n",
        "\n",
        "    # criterion = nn.MSELoss()\n",
        "    # Replace MSELoss with SmoothL1Loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "\n",
        "    # Example weights (adjust as needed)\n",
        "    w_sa, w_sv = 1.0, 1.2\n",
        "\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    test_losses = []  # für jede epoch den test loss speichern aber halt auch den g (Max is Deutsch)\n",
        "    epoch_times = []  # Track epoch durations\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_train_loss = 0.0\n",
        "        for Xb, yb in train_loader:\n",
        "            # Xb, yb = Xb.to(device), yb.to(device)\n",
        "            # optimizer.zero_grad()\n",
        "            # preds = model(Xb)\n",
        "            # loss = criterion(preds, yb)\n",
        "            # loss.backward()\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            # optimizer.step()\n",
        "            # epoch_train_loss += loss.item() * Xb.size(0)\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(Xb)\n",
        "            # Compute per-target loss\n",
        "            loss_sa = criterion(preds[:, 0], yb[:, 0])\n",
        "            loss_sv = criterion(preds[:, 1], yb[:, 1])\n",
        "            loss = w_sa * loss_sa + w_sv * loss_sv\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            epoch_train_loss += loss.item() * Xb.size(0)\n",
        "        train_losses.append(epoch_train_loss / len(train_loader.dataset))\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        epoch_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for Xb, yb in val_loader:\n",
        "                Xb, yb = Xb.to(device), yb.to(device)\n",
        "                epoch_val_loss += criterion(model(Xb), yb).item() * Xb.size(0)\n",
        "        val_losses.append(epoch_val_loss / len(val_loader.dataset))\n",
        "\n",
        "        # TEST PHASE - NOW EVALUATED EVERY EPOCH\n",
        "        epoch_test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for Xb, yb in test_loader:\n",
        "                Xb, yb = Xb.to(device), yb.to(device)\n",
        "                epoch_test_loss += criterion(model(Xb), yb).item() * Xb.size(0)\n",
        "        test_losses.append(epoch_test_loss / len(test_loader.dataset))\n",
        "\n",
        "        # Calculate epoch duration and ETA; einfach nur für den aktuellen Epoch\n",
        "        epoch_duration = time.time() - epoch_start_time\n",
        "        epoch_times.append(epoch_duration)\n",
        "\n",
        "        # Calculate ETA based on average epoch time\n",
        "        avg_epoch_time = np.mean(epoch_times)\n",
        "        remaining_epochs = num_epochs - epoch\n",
        "        eta_seconds = remaining_epochs * avg_epoch_time\n",
        "        eta_mins = eta_seconds / 60\n",
        "\n",
        "        # Update scheduler and early stopping\n",
        "        scheduler.step(val_losses[-1])\n",
        "        if val_losses[-1] < best_val_loss:\n",
        "            best_val_loss = val_losses[-1]\n",
        "            patience_counter = 0\n",
        "            status = ''\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            status = '⚪️'\n",
        "            if patience_counter >= n_patience:\n",
        "                print(f\"\\nEarly stopping triggered at epoch {epoch}!\")\n",
        "                break\n",
        "\n",
        "        print(f\"{status} {model_name} Epoch {epoch:3d}: Train {train_losses[-1]:.4f} | Val {val_losses[-1]:.4f} | Test {test_losses[-1]:.4f} | {epoch_duration:.1f}s/epoch | ETA: {eta_mins:.1f}mins ({eta_seconds:.0f}s)\")\n",
        "\n",
        "    print(f\"\\n{model_name} Training complete!\")\n",
        "    print(f\"Best val loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    # Save model and training history\n",
        "    torch.save({\n",
        "        'model_state': model.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'test_losses': test_losses,  # Full test loss trajectory - weil wir es jetzt jedes Epoch speichern\n",
        "        'config': {\n",
        "            'num_channels': num_channels,\n",
        "            'window_size': window_size,\n",
        "            'model_name': model_name\n",
        "        }\n",
        "    }, os.path.join(save_dir, f'{model_name}.pt'))\n",
        "\n",
        "print(\"\\nAll models trained and saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DOGZRxurN93",
        "outputId": "b85feff0-f3cf-4821-e763-2da4ffb0da73"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Using GPU: Tesla T4\n",
            "\n",
            "=== Training LightTransformer ===\n",
            " LightTransformer Epoch   1: Train 0.0163 | Val 0.0012 | Test 0.0036 | 2.2s/epoch | ETA: 0.5mins (31s)\n",
            " LightTransformer Epoch   2: Train 0.0022 | Val 0.0006 | Test 0.0033 | 2.0s/epoch | ETA: 0.5mins (27s)\n",
            " LightTransformer Epoch   3: Train 0.0012 | Val 0.0004 | Test 0.0036 | 2.0s/epoch | ETA: 0.4mins (25s)\n",
            " LightTransformer Epoch   4: Train 0.0009 | Val 0.0002 | Test 0.0030 | 2.0s/epoch | ETA: 0.4mins (23s)\n",
            " LightTransformer Epoch   5: Train 0.0006 | Val 0.0002 | Test 0.0032 | 2.1s/epoch | ETA: 0.3mins (21s)\n",
            "⚪️ LightTransformer Epoch   6: Train 0.0005 | Val 0.0002 | Test 0.0034 | 2.1s/epoch | ETA: 0.3mins (19s)\n",
            "⚪️ LightTransformer Epoch   7: Train 0.0004 | Val 0.0002 | Test 0.0034 | 2.2s/epoch | ETA: 0.3mins (17s)\n",
            " LightTransformer Epoch   8: Train 0.0004 | Val 0.0002 | Test 0.0028 | 2.1s/epoch | ETA: 0.2mins (15s)\n",
            " LightTransformer Epoch   9: Train 0.0003 | Val 0.0001 | Test 0.0031 | 2.0s/epoch | ETA: 0.2mins (12s)\n",
            "⚪️ LightTransformer Epoch  10: Train 0.0003 | Val 0.0002 | Test 0.0030 | 2.1s/epoch | ETA: 0.2mins (10s)\n",
            "⚪️ LightTransformer Epoch  11: Train 0.0003 | Val 0.0002 | Test 0.0031 | 2.0s/epoch | ETA: 0.1mins (8s)\n",
            "⚪️ LightTransformer Epoch  12: Train 0.0002 | Val 0.0001 | Test 0.0028 | 2.0s/epoch | ETA: 0.1mins (6s)\n",
            "⚪️ LightTransformer Epoch  13: Train 0.0002 | Val 0.0002 | Test 0.0031 | 2.1s/epoch | ETA: 0.1mins (4s)\n",
            " LightTransformer Epoch  14: Train 0.0002 | Val 0.0001 | Test 0.0031 | 2.0s/epoch | ETA: 0.0mins (2s)\n",
            " LightTransformer Epoch  15: Train 0.0002 | Val 0.0000 | Test 0.0028 | 2.0s/epoch | ETA: 0.0mins (0s)\n",
            "\n",
            "LightTransformer Training complete!\n",
            "Best val loss: 0.0000\n",
            "\n",
            "All models trained and saved!\n"
          ]
        }
      ]
    }
  ]
}