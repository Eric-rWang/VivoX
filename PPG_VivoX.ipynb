{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eric-rWang/VivoX/blob/main/PPG_VivoX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PNnmqXCt43U",
        "outputId": "341ff8d9-8dde-4c91-c8fb-11644c06cc57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PPG ML\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import h5py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "os.chdir(\"/content/PPG ML\")\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U9WdLqoS3XT"
      },
      "source": [
        "VivoX transformer model:\n",
        "<ul>\n",
        "<li>Linear projection from flattened window inputs to d_model</li>\n",
        "<li>Sinusoidal positional encodings</li>\n",
        "<li>Standard encoder stack</li>\n",
        "<li>Mean pooling and MLP head for arterial and venous regression</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "MXdPytBLS2O2"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements sinusoidal positional encoding.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                             (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
        "        Returns:\n",
        "            Tensor: (batch_size, seq_len, d_model) with positional encoding added\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "\n",
        "class vivoxTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Transformer model for arterial & venous SpO2 prediction from multichannel PPG.\n",
        "\n",
        "    Args:\n",
        "        num_channels: number of input channels (e.g., 36)\n",
        "        window_size: number of time samples per window (e.g., 350)\n",
        "        d_model: embedding dimension\n",
        "        nhead: number of attention heads\n",
        "        num_layers: number of Transformer encoder layers\n",
        "        dim_feedforward: dimension of the MLP in encoder layers\n",
        "        dropout: dropout rate\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_channels: int = 36,\n",
        "        window_size: int = 350,\n",
        "        d_model: int = 128,\n",
        "        nhead: int = 4,\n",
        "        num_layers: int = 3,\n",
        "        dim_feedforward: int = 512,\n",
        "        dropout: float = 0.1, # Increased from 0.1\n",
        "        max_windows: int = 200,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        input_dim = num_channels * window_size\n",
        "\n",
        "        # 1) Linear projection of each window to d_model\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        # self.input_norm = nn.LayerNorm(d_model) # Normalize\n",
        "\n",
        "        # 2) Positional encoding for up to max_windows\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len=max_windows)\n",
        "\n",
        "        # 3) Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True  # allows (batch, seq, feature)\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "        # 4) Regression head for arterial & venous SpO2\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(dropout),\n",
        "            nn.Linear(d_model // 2, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, num_windows, num_channels, window_size)\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, 2) containing [SpO2_art, SpO2_ven]\n",
        "        \"\"\"\n",
        "        batch, num_windows, C, W = x.size()\n",
        "        # Flatten channels & time\n",
        "        x = x.view(batch, num_windows, C * W)             # (batch, num_windows, input_dim)\n",
        "\n",
        "        # Embed\n",
        "        x = self.input_proj(x)                           # (batch, num_windows, d_model)\n",
        "\n",
        "        # Normalize\n",
        "        # x = self.input_norm(x)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.pos_enc(x)                              # (batch, num_windows, d_model)\n",
        "\n",
        "        # Transformer encoding\n",
        "        x = self.transformer(x)                          # (batch, num_windows, d_model)\n",
        "\n",
        "        # Pool across time windows\n",
        "        rep = x.mean(dim=1)                              # (batch, d_model)\n",
        "\n",
        "        # Predict SpO2\n",
        "        out = self.head(rep)                             # (batch, 2)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USXvnJ2zuagH"
      },
      "source": [
        "Loading in data set from phantom setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "CvwTXMWBufVy"
      },
      "outputs": [],
      "source": [
        "def import_h5py(file_path):\n",
        "  with h5py.File(file_path, 'r') as f:\n",
        "      X = f['waveforms'][:]\n",
        "      y = f['labels'][:]\n",
        "\n",
        "      return X, y\n",
        "\n",
        "file_path = \"Shifting_Position_2.h5\"\n",
        "combined_data, labels_array = import_h5py(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2g9qTNCxtxE"
      },
      "source": [
        "Splitting imported dataset into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUn-ftNrx2qf",
        "outputId": "7dd64d6a-7eac-4e17-d3e2-ca2ef023a75e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (2126, 350, 36)\n",
            "X_test  shape: (532, 350, 36)\n"
          ]
        }
      ],
      "source": [
        "labels_df = pd.DataFrame(\n",
        "        labels_array,\n",
        "        columns=['arterial_saturation', 'venous_saturation']\n",
        ")\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    combined_data, labels_array, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Print out shape of train and test data\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test  shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwNol1BLyguH"
      },
      "source": [
        "Training CNN function. Setting epoch to 500 for now. Since channels of data have their own unique position, we add positional as well as wavelength embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "w0hhMbqozGIN"
      },
      "outputs": [],
      "source": [
        "save_path = 'vivoxTransformer_2.pth'\n",
        "\n",
        "# --- Hyperparameters & settings ---\n",
        "num_channels = 36\n",
        "window_size  = 350\n",
        "d_model      = 128\n",
        "batch_size   = 16\n",
        "lr           = 1e-4\n",
        "weight_decay = 5e-5\n",
        "num_epochs   = int(10e5)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Convert NumPy to torch tensors (no permute needed; model expects (B, T, C))\n",
        "X_train_t = torch.from_numpy(X_train).float()\n",
        "y_train_t = torch.from_numpy(y_train).float()\n",
        "X_test_t  = torch.from_numpy(X_test).float()\n",
        "y_test_t  = torch.from_numpy(y_test).float()\n",
        "\n",
        "# 2) permute & unsqueeze so each sample is (1 window, 36 channels, 350 timesteps)\n",
        "#    result: (N, 1, 36, 350)\n",
        "X_train_rs = X_train_t.permute(0, 2, 1).unsqueeze(1)\n",
        "X_test_rs  = X_test_t .permute(0, 2, 1).unsqueeze(1)\n",
        "# assuming X_train_rs is (N,1,36,350)\n",
        "mean = X_train_rs.mean(dim=(0,1,3), keepdim=True)  # per-channel mean\n",
        "std  =  X_train_rs.std(dim=(0,1,3), keepdim=True)  # per-channel std\n",
        "X_train_rs = (X_train_rs - mean) / (std + 1e-6)\n",
        "X_test_rs  = (X_test_rs  - mean) / (std + 1e-6)\n",
        "\n",
        "# 3) DataLoaders\n",
        "train_loader = DataLoader(TensorDataset(X_train_rs, y_train_t),\n",
        "                          batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(TensorDataset(X_test_rs,  y_test_t),\n",
        "                          batch_size=batch_size)"
      ],
      "metadata": {
        "id": "69iLiWg68W9s"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "OqyCrCHpT8DG"
      },
      "outputs": [],
      "source": [
        "# 2) Model Setup\n",
        "model = vivoxTransformer(\n",
        "            num_channels=num_channels,\n",
        "            window_size=window_size,\n",
        "            d_model=d_model\n",
        "        ).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8uii8lU0T-Dl",
        "outputId": "7ea86d18-2b2a-4bef-cde8-87cf3de98316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training Loop\n",
            "Epoch   1: Train 3133.4368, Test 2852.0312 → saved best\n",
            "Epoch   2: Train 2511.9489, Test 2094.9815 → saved best\n",
            "Epoch   3: Train 1692.7130, Test 1258.0442 → saved best\n",
            "Epoch   4: Train 940.0823, Test 627.9647 → saved best\n",
            "Epoch   5: Train 444.3129, Test 269.2789 → saved best\n",
            "Epoch   6: Train 188.7490, Test 119.1112 → saved best\n",
            "Epoch   7: Train 97.5555, Test 79.9611 → saved best\n",
            "Epoch   8: Train 77.4371, Test 75.1580 → saved best\n",
            "Epoch   9: Train 71.5655, Test 66.9759 → saved best\n",
            "Epoch  10: Train 56.5969, Test 55.0676 → saved best\n",
            "Epoch  11: Train 47.3052, Test 54.8895 → saved best\n",
            "Epoch  12: Train 32.2889, Test 36.8815 → saved best\n",
            "Epoch  13: Train 28.5005, Test 25.9819 → saved best\n",
            "Epoch  14: Train 18.3778, Test 20.5975 → saved best\n",
            "Epoch  15: Train 16.3965, Test 22.6806 → saved best\n",
            "Epoch  16: Train 17.4104, Test 15.3045 \n",
            "Epoch  17: Train 14.4412, Test 16.1248 → saved best\n",
            "Epoch  18: Train 13.5208, Test 14.3304 → saved best\n",
            "Epoch  19: Train 12.9476, Test 14.4341 → saved best\n",
            "Epoch  20: Train 12.4016, Test 13.4624 → saved best\n",
            "Epoch  21: Train 11.9442, Test 11.8056 → saved best\n",
            "Epoch  22: Train 11.4388, Test 11.8967 → saved best\n",
            "Epoch  23: Train 10.5453, Test 10.9683 → saved best\n",
            "Epoch  24: Train 9.5123, Test 9.9903 → saved best\n",
            "Epoch  25: Train 8.7976, Test 9.6235 → saved best\n",
            "Epoch  26: Train 8.2842, Test 10.1139 → saved best\n",
            "Epoch  27: Train 8.0038, Test 8.0838 → saved best\n",
            "Epoch  28: Train 7.7011, Test 7.5382 → saved best\n",
            "Epoch  29: Train 7.4569, Test 7.9423 → saved best\n",
            "Epoch  30: Train 6.9591, Test 8.4291 → saved best\n",
            "Epoch  31: Train 6.3890, Test 6.4286 → saved best\n",
            "Epoch  32: Train 5.7600, Test 6.4672 → saved best\n",
            "Epoch  33: Train 5.2443, Test 7.8681 → saved best\n",
            "Epoch  34: Train 9.2323, Test 22.0398 \n",
            "Epoch  35: Train 15.0797, Test 7.8783 \n",
            "Epoch  36: Train 7.4532, Test 15.2287 \n",
            "Epoch  37: Train 3.4415, Test 13.7460 → saved best\n",
            "Epoch  38: Train 2.7765, Test 14.0535 → saved best\n",
            "Epoch  39: Train 3.4648, Test 16.2175 \n",
            "Epoch  40: Train 2.8747, Test 11.0943 \n",
            "Epoch  41: Train 3.0390, Test 16.8930 \n",
            "Epoch  42: Train 0.8337, Test 16.4733 → saved best\n",
            "Epoch  43: Train 0.5004, Test 12.0220 → saved best\n",
            "Epoch  44: Train 0.4376, Test 9.8302 → saved best\n",
            "Epoch  45: Train 0.3582, Test 9.4734 → saved best\n",
            "Epoch  46: Train 0.2708, Test 8.1947 → saved best\n",
            "Epoch  47: Train 0.2093, Test 7.8077 → saved best\n",
            "Epoch  48: Train 0.2137, Test 7.9461 \n",
            "Epoch  49: Train 0.1977, Test 6.8278 → saved best\n",
            "Epoch  50: Train 0.2090, Test 1.9376 \n",
            "Epoch  51: Train 0.2074, Test 2.1535 \n",
            "Epoch  52: Train 0.1833, Test 1.7343 → saved best\n",
            "Epoch  53: Train 0.1955, Test 1.7497 \n",
            "Epoch  54: Train 0.1480, Test 1.7059 → saved best\n",
            "Epoch  55: Train 0.1650, Test 1.4541 \n",
            "Epoch  56: Train 0.1368, Test 1.5615 → saved best\n",
            "Epoch  57: Train 0.1302, Test 1.6407 → saved best\n",
            "Epoch  58: Train 0.1318, Test 1.3623 \n",
            "Epoch  59: Train 0.1124, Test 1.3821 → saved best\n",
            "Epoch  60: Train 0.1341, Test 1.3773 \n",
            "Epoch  61: Train 0.1552, Test 1.2954 \n",
            "Epoch  62: Train 0.1283, Test 1.1263 \n",
            "Epoch  63: Train 0.1475, Test 1.0134 \n",
            "Epoch  64: Train 0.1223, Test 1.0308 \n",
            "Epoch  65: Train 0.1256, Test 1.1144 \n",
            "Epoch  66: Train 0.1078, Test 1.1615 → saved best\n",
            "Epoch  67: Train 0.1064, Test 6.6494 → saved best\n",
            "Epoch  68: Train 0.1397, Test 6.3833 \n",
            "Epoch  69: Train 0.1019, Test 7.2370 → saved best\n",
            "Epoch  70: Train 0.1494, Test 8.8595 \n",
            "Epoch  71: Train 0.1019, Test 6.8823 → saved best\n",
            "Epoch  72: Train 0.1437, Test 4.5315 \n",
            "Epoch  73: Train 0.0977, Test 3.5673 → saved best\n",
            "Epoch  74: Train 0.0928, Test 2.4201 → saved best\n",
            "Epoch  75: Train 0.1040, Test 1.3179 \n",
            "Epoch  76: Train 0.0799, Test 1.0874 → saved best\n",
            "Epoch  77: Train 0.1060, Test 1.1618 \n",
            "Epoch  78: Train 0.0809, Test 0.8231 \n",
            "Epoch  79: Train 0.0993, Test 1.1669 \n",
            "Epoch  80: Train 0.0933, Test 0.9340 \n",
            "Epoch  81: Train 0.0827, Test 0.9577 \n",
            "Epoch  82: Train 0.0960, Test 1.0018 \n",
            "Epoch  83: Train 0.0951, Test 1.4093 \n",
            "Epoch  84: Train 0.0835, Test 1.1234 \n",
            "Epoch  85: Train 0.0803, Test 1.2656 \n",
            "Epoch  86: Train 0.0886, Test 1.4933 \n",
            "Epoch  87: Train 0.0828, Test 1.5103 \n",
            "Epoch  88: Train 0.0831, Test 1.4718 \n",
            "Epoch  89: Train 0.0636, Test 1.3544 → saved best\n",
            "Epoch  90: Train 0.0644, Test 1.3351 \n",
            "Epoch  91: Train 0.0773, Test 1.3128 \n",
            "Epoch  92: Train 0.0825, Test 1.1457 \n",
            "Epoch  93: Train 0.0856, Test 2.0987 \n",
            "Epoch  94: Train 0.1002, Test 5.5125 \n",
            "Epoch  95: Train 0.0888, Test 11.6734 \n",
            "Epoch  96: Train 7.0336, Test 30.0393 \n",
            "Epoch  97: Train 16.5231, Test 33.4625 \n",
            "Epoch  98: Train 3.0932, Test 16.8048 \n",
            "Epoch  99: Train 4.2163, Test 11.7339 \n",
            "Epoch 100: Train 0.1862, Test 11.8532 \n",
            "Epoch 101: Train 0.1597, Test 9.9357 \n",
            "Epoch 102: Train 0.1160, Test 8.5527 \n",
            "Epoch 103: Train 0.0945, Test 7.9514 \n",
            "Epoch 104: Train 0.0934, Test 8.1508 \n",
            "Epoch 105: Train 0.1151, Test 8.4550 \n",
            "Epoch 106: Train 0.0761, Test 8.4517 \n",
            "Epoch 107: Train 0.0872, Test 7.7003 \n",
            "Epoch 108: Train 0.0816, Test 7.8313 \n",
            "Epoch 109: Train 0.0736, Test 7.9476 \n",
            "Epoch 110: Train 0.0888, Test 8.0350 \n",
            "Epoch 111: Train 0.0811, Test 8.2414 \n",
            "Epoch 112: Train 0.1693, Test 4.5010 \n",
            "Epoch 113: Train 2.9192, Test 18.9580 \n",
            "Epoch 114: Train 0.1022, Test 7.0140 \n",
            "Epoch 115: Train 0.0819, Test 4.7485 \n",
            "Epoch 116: Train 0.0826, Test 4.7627 \n",
            "Epoch 117: Train 0.0779, Test 1.6084 \n",
            "Epoch 118: Train 0.0775, Test 1.4024 \n",
            "Epoch 119: Train 0.0881, Test 1.8317 \n",
            "Epoch 120: Train 0.0743, Test 1.9771 \n",
            "Epoch 121: Train 0.0595, Test 2.0807 → saved best\n",
            "Epoch 122: Train 0.0533, Test 2.2376 → saved best\n",
            "Epoch 123: Train 0.0729, Test 2.2959 \n",
            "Epoch 124: Train 0.0630, Test 2.2972 \n",
            "Epoch 125: Train 0.0643, Test 2.5433 \n",
            "Epoch 126: Train 0.0667, Test 2.1951 \n",
            "Epoch 127: Train 0.0602, Test 2.5085 \n",
            "Epoch 128: Train 0.0657, Test 2.6278 \n",
            "Epoch 129: Train 0.0628, Test 2.6992 \n",
            "Epoch 130: Train 0.0582, Test 2.8792 \n",
            "Epoch 131: Train 0.0570, Test 2.7997 \n",
            "Epoch 132: Train 0.0518, Test 2.1562 → saved best\n",
            "Epoch 133: Train 0.0634, Test 2.1764 \n",
            "Epoch 134: Train 0.0649, Test 2.1209 \n",
            "Epoch 135: Train 0.0593, Test 2.1547 \n",
            "Epoch 136: Train 0.0545, Test 2.3688 \n",
            "Epoch 137: Train 0.0518, Test 2.3147 → saved best\n",
            "Epoch 138: Train 0.0713, Test 1.7429 \n",
            "Epoch 139: Train 0.0678, Test 1.5138 \n",
            "Epoch 140: Train 0.0564, Test 1.5677 \n",
            "Epoch 141: Train 0.0584, Test 1.5491 \n",
            "Epoch 142: Train 0.0616, Test 1.1867 \n",
            "Epoch 143: Train 0.0642, Test 1.2896 \n",
            "Epoch 144: Train 0.0460, Test 1.4701 → saved best\n",
            "Epoch 145: Train 0.0511, Test 0.9426 \n",
            "Epoch 146: Train 0.0558, Test 1.0442 \n",
            "Epoch 147: Train 0.0472, Test 1.1036 \n",
            "Epoch 148: Train 0.0534, Test 1.1668 \n",
            "Epoch 149: Train 0.0528, Test 1.4483 \n",
            "Epoch 150: Train 0.0545, Test 1.3828 \n",
            "Epoch 151: Train 0.0574, Test 1.1930 \n",
            "Epoch 152: Train 0.0579, Test 0.6363 \n",
            "Epoch 153: Train 0.0494, Test 0.6316 \n",
            "Epoch 154: Train 3.4123, Test 23.5821 \n",
            "Epoch 155: Train 29.5178, Test 22.5653 \n",
            "Epoch 156: Train 7.6530, Test 21.3067 \n",
            "Epoch 157: Train 4.3641, Test 4.6552 \n",
            "Epoch 158: Train 1.2753, Test 3.4836 \n",
            "Epoch 159: Train 0.8937, Test 8.4141 \n",
            "Epoch 160: Train 6.9265, Test 21.2275 \n",
            "Epoch 161: Train 1.7802, Test 13.7979 \n",
            "Epoch 162: Train 1.1779, Test 16.4795 \n",
            "Epoch 163: Train 1.5926, Test 7.4314 \n",
            "Epoch 164: Train 1.1952, Test 9.4885 \n",
            "Epoch 165: Train 0.1676, Test 10.3025 \n",
            "Epoch 166: Train 0.1055, Test 8.2428 \n",
            "Epoch 167: Train 0.1208, Test 7.9283 \n",
            "Epoch 168: Train 0.0873, Test 7.8485 \n",
            "Epoch 169: Train 0.0979, Test 7.0617 \n",
            "Epoch 170: Train 0.0865, Test 6.9981 \n",
            "Epoch 171: Train 0.0859, Test 7.0663 \n",
            "Epoch 172: Train 0.0861, Test 7.0901 \n",
            "Epoch 173: Train 0.0737, Test 6.9089 \n",
            "Epoch 174: Train 0.0822, Test 6.0576 \n",
            "Epoch 175: Train 0.0656, Test 5.5026 \n",
            "Epoch 176: Train 0.0698, Test 5.0934 \n",
            "Epoch 177: Train 0.0658, Test 5.1032 \n",
            "Epoch 178: Train 0.0624, Test 5.2363 \n",
            "Epoch 179: Train 0.0644, Test 5.0534 \n",
            "Epoch 180: Train 0.0757, Test 4.6385 \n",
            "Epoch 181: Train 0.0858, Test 4.6859 \n",
            "Epoch 182: Train 0.0602, Test 4.4411 \n",
            "Epoch 183: Train 0.0726, Test 4.2388 \n",
            "Epoch 184: Train 0.0540, Test 4.2249 \n",
            "Epoch 185: Train 0.0629, Test 4.3125 \n",
            "Epoch 186: Train 0.0589, Test 4.1004 \n",
            "Epoch 187: Train 0.0677, Test 4.7123 \n",
            "Epoch 188: Train 0.0693, Test 3.0957 \n",
            "Epoch 189: Train 0.0760, Test 2.9149 \n",
            "Epoch 190: Train 0.0583, Test 2.6988 \n",
            "Epoch 191: Train 0.0575, Test 2.4270 \n",
            "Epoch 192: Train 0.0584, Test 2.1230 \n",
            "Epoch 193: Train 0.0530, Test 2.0233 \n",
            "Epoch 194: Train 0.0572, Test 1.7825 \n",
            "Epoch 195: Train 0.0637, Test 1.8277 \n",
            "Epoch 196: Train 0.0591, Test 1.8176 \n",
            "Epoch 197: Train 0.0496, Test 1.5454 \n",
            "Epoch 198: Train 0.0513, Test 1.5555 \n",
            "Epoch 199: Train 0.0521, Test 1.6416 \n",
            "Epoch 200: Train 0.0635, Test 2.4733 \n",
            "Epoch 201: Train 0.0636, Test 2.2822 \n",
            "Epoch 202: Train 0.0684, Test 1.5821 \n",
            "Epoch 203: Train 0.0506, Test 1.4974 \n",
            "Epoch 204: Train 0.0487, Test 1.2941 \n",
            "Epoch 205: Train 0.0517, Test 0.9732 \n",
            "Epoch 206: Train 0.0488, Test 0.8309 \n",
            "Epoch 207: Train 0.0535, Test 0.7738 \n",
            "Epoch 208: Train 0.0737, Test 0.8347 \n",
            "Epoch 209: Train 0.0546, Test 0.6435 \n",
            "Epoch 210: Train 0.0469, Test 0.5924 \n",
            "Epoch 211: Train 0.0464, Test 0.4771 \n",
            "Epoch 212: Train 0.0545, Test 0.7733 \n",
            "Epoch 213: Train 0.0497, Test 0.5633 \n",
            "Epoch 214: Train 0.0485, Test 0.5186 \n",
            "Epoch 215: Train 0.0440, Test 0.5170 → saved best\n",
            "Epoch 216: Train 0.0463, Test 0.5135 \n",
            "Epoch 217: Train 0.0420, Test 0.5528 → saved best\n",
            "Epoch 218: Train 0.0449, Test 0.5238 \n",
            "Epoch 219: Train 0.0431, Test 0.4741 \n",
            "Epoch 220: Train 0.0448, Test 0.7959 \n",
            "Epoch 221: Train 0.0357, Test 0.8098 → saved best\n",
            "Epoch 222: Train 0.0384, Test 0.7978 \n",
            "Epoch 223: Train 0.0384, Test 0.6994 \n",
            "Epoch 224: Train 0.0484, Test 0.7778 \n",
            "Epoch 225: Train 0.0349, Test 0.7180 → saved best\n",
            "Epoch 226: Train 0.0372, Test 0.4379 \n",
            "Epoch 227: Train 0.8318, Test 9.6268 \n",
            "Epoch 228: Train 14.4823, Test 43.9483 \n",
            "Epoch 229: Train 12.6007, Test 13.7100 \n",
            "Epoch 230: Train 8.9288, Test 13.9177 \n",
            "Epoch 231: Train 4.6981, Test 12.6650 \n",
            "Epoch 232: Train 2.9298, Test 13.6034 \n",
            "Epoch 233: Train 1.3597, Test 7.4267 \n",
            "Epoch 234: Train 0.1495, Test 7.1849 \n",
            "Epoch 235: Train 0.1565, Test 6.7161 \n",
            "Epoch 236: Train 0.1104, Test 7.1294 \n",
            "Epoch 237: Train 0.0800, Test 6.9283 \n",
            "Epoch 238: Train 0.0588, Test 6.8959 \n",
            "Epoch 239: Train 0.0566, Test 6.8367 \n",
            "Epoch 240: Train 0.0647, Test 6.9743 \n",
            "Epoch 241: Train 0.0574, Test 6.7770 \n",
            "Epoch 242: Train 0.0578, Test 6.6915 \n",
            "Epoch 243: Train 0.0598, Test 6.5605 \n",
            "Epoch 244: Train 0.0530, Test 6.6276 \n",
            "Epoch 245: Train 0.0538, Test 6.6032 \n",
            "Epoch 246: Train 0.0562, Test 6.6238 \n",
            "Epoch 247: Train 0.0603, Test 6.8196 \n",
            "Epoch 248: Train 0.0496, Test 6.7770 \n",
            "Epoch 249: Train 0.0546, Test 6.8591 \n",
            "Epoch 250: Train 0.0490, Test 6.7792 \n",
            "Epoch 251: Train 0.0574, Test 6.8450 \n",
            "Epoch 252: Train 0.0448, Test 6.4920 \n",
            "Epoch 253: Train 0.0477, Test 6.5565 \n",
            "Epoch 254: Train 0.0410, Test 6.5894 \n",
            "Epoch 255: Train 0.0411, Test 6.5137 \n",
            "Epoch 256: Train 0.0466, Test 6.5857 \n",
            "Epoch 257: Train 0.0427, Test 6.6305 \n",
            "Epoch 258: Train 0.0397, Test 6.5448 \n",
            "Epoch 259: Train 0.0458, Test 6.5558 \n",
            "Epoch 260: Train 0.0451, Test 7.3218 \n",
            "Epoch 261: Train 0.0427, Test 6.7475 \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-68-1314512105.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Xb is (batch, 1, 36, 350)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpreds\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[0;34m)\u001b[0m                      \u001b[0;31m# no more unpacking error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "best_train_loss = float('inf')\n",
        "print('Starting Training Loop')\n",
        "\n",
        "# 4) Training loop with device‐moves\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # -- Train --\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)    # Xb is (batch, 1, 36, 350)\n",
        "        preds  = model(Xb)                      # no more unpacking error\n",
        "        loss   = criterion(preds, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * Xb.size(0)\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # -- Evaluate --\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in test_loader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            test_loss += criterion(model(Xb), yb).item() * Xb.size(0)\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    # -- Checkpoint --\n",
        "    if train_loss < best_train_loss:\n",
        "        best_train_loss = train_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        status = '→ saved best'\n",
        "    else:\n",
        "        status = ''\n",
        "    print(f\"Epoch {epoch:3d}: Train {train_loss:.4f}, Test {test_loss:.4f} {status}\")\n",
        "\n",
        "print(\"Training complete. Best test loss:\", best_train_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}